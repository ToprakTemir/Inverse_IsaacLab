program: assembly_by_inverse/train_inverse_skill.py
method: bayes
metric:
  name: eval/mean_reward
  goal: maximize

early_terminate:
  type: hyperband
  min_iter: 3

parameters:
  # --------- Data ---------
  demo_path:
    value: ./datasets/disassembly_15.hdf5
  val_demo_path:
    value: ./datasets/disassembly_validation_5.hdf5

  # --------- Stage toggles (sweep can enable/disable) ---------
  train_phase:
    values: [true]   # set to [true, false] to allow loading a fixed model
  train_bc:
    values: [true]
  train_ppo:
    values: [true]

  # Optional load paths if toggles are false
  phase_load_path:
    value: models/2025-08-28-17:45/phase_evaluator_best.pth
  phase_obs_dim:
    value: 9999999
  bc_load_path:
    value: models/2025-08-27-20:07/bc_policy_best.pth
  bc_obs_dim:
    value: 25
  bc_act_dim:
    value: 7

  # --------- Feature selection (post-robot slice) ---------
  non_robot_start:
    value: 8

  # --------- PhaseEvaluator ---------
  phase_hidden:
    values:
      - [128, 128]
      - [256, 256]
      - [512, 256]
      - [512, 512]
      - [1024, 512]
  phase_lr:
    distribution: log_uniform_values
    min: 1.0e-4
    max: 5.0e-3
  phase_batch:
    values: [128, 256, 512]
  phase_epochs:
    values: [5000, 10000, 15000]
  phase_tv_weight:
    distribution: uniform
    min: 0.05
    max: 0.25
  phase_val_period:
    values: [50, 100, 200]
  phase_patience:
    values: [400, 800, 1200]

  # --------- BC Policy ---------
  bc_hidden:
    values:
      - [128, 128]
      - [256, 256]
      - [512, 256]
      - [512, 512]
      - [1024, 512]
  bc_lr:
    distribution: log_uniform_values
    min: 1.0e-4
    max: 5.0e-3
  bc_batch:
    values: [128, 256, 512]
  bc_epochs:
    values: [5000, 20000, 50000]
  bc_action_offset:
    values: [1, 3, 5, 7]
  bc_reverse_time:
    values: [true, false]
  bc_val_period:
    values: [50, 100, 200]
  bc_patience:
    values: [200, 500, 800]

  # --------- PPO (SB3) ---------
  ppo_n_steps:
    values: [128, 256, 512, 1024]
  ppo_batch_size:
    values: [128, 256, 512, 1024]
  ppo_lr:
    distribution: log_uniform_values
    min: 1.0e-5
    max: 3.0e-4
  ppo_ent_coef:
    distribution: log_uniform_values
    min: 1.0e-4
    max: 1.0e-1
  ppo_vf_coef:
    values: [0.5, 1.0, 1.5]
  ppo_gamma:
    values: [0.97, 0.98, 0.99, 0.995]
  ppo_gae_lambda:
    values: [0.9, 0.95, 0.97]
  ppo_clip_range:
    values: [0.1, 0.2, 0.3]

  # --------- Reward shaping ---------
  reward_weight:
    distribution: log_uniform_values
    min: 0.1
    max: 5.0

  # --------- Runtime / logging ---------
  total_timesteps:
    value: 3000000   # adjust if you want shorter/faster sweeps
  eval_every:
    value: 10
  ckpt_every:
    value: 10
  early_stop_patience:
    value: 20
